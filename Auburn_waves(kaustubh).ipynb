{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing, Model Selection and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import cv2\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "num_classes = 8  \n",
    "\n",
    "# Loading the InceptionV3 model pre-trained on ImageNet, excluding the top layers\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Adding custom layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x) \n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freezing/unfreezing the layers of the base model for fine tuning\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "model.compile(optimizer=Adam(learning_rate=initial_learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reading the CSV file\n",
    "data_df = pd.read_csv(r'D:\\Academics\\IIT\\Tech\\COSMOSOC\\SOI-sds\\train_dataset\\train.csv')\n",
    "image_dir = r'D:\\Academics\\IIT\\Tech\\COSMOSOC\\SOI-sds\\train_dataset\\train_dataset'\n",
    "\n",
    "image_files = os.listdir(image_dir)\n",
    "image_files = [os.path.splitext(filename)[0] for filename in image_files]\n",
    "\n",
    "# Filter the DataFrame to include only existing image filenames\n",
    "data_df = data_df[data_df['File_Name'].apply(lambda x: os.path.splitext(x)[0] in image_files)]\n",
    "\n",
    "data_df.to_csv(r'D:\\Academics\\IIT\\Tech\\COSMOSOC\\SOI-sds\\train_dataset\\train.csv', index=False)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data_df['label'] = label_encoder.fit_transform(data_df['label'])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom data generator(Data augmentation part)\n",
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, df, batch_size, image_size, image_dir, num_classes, augment=True):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.image_dir = image_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.augment = augment\n",
    "        self.indices = np.arange(len(self.df))\n",
    "        self.on_epoch_end()\n",
    "        self.datagen = ImageDataGenerator(\n",
    "            rescale=1.0/255.0,\n",
    "            shear_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            rotation_range=10,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1\n",
    "        ) if self.augment else ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_df = self.df.iloc[batch_indices]\n",
    "        images = np.empty((self.batch_size, *self.image_size, 3), dtype=np.float32)\n",
    "        labels = np.empty((self.batch_size, self.num_classes), dtype=np.float32)\n",
    "\n",
    "        for i, (_, row) in enumerate(batch_df.iterrows()):\n",
    "            image_path = os.path.join(self.image_dir, row['File_Name'])\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Warning: Could not read image {image_path}. Skipping...\")\n",
    "                continue\n",
    "            image = cv2.resize(image, self.image_size)\n",
    "            if self.augment:\n",
    "                image = self.datagen.random_transform(image)\n",
    "            images[i] = image\n",
    "            labels[i] = tf.keras.utils.to_categorical(row['label'], num_classes=self.num_classes)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "image_size = (224, 224)\n",
    "\n",
    "\n",
    "train_generator = CustomDataGenerator(train_df, batch_size, image_size, image_dir, num_classes, augment=True)\n",
    "val_generator = CustomDataGenerator(val_df, batch_size, image_size, image_dir, num_classes, augment=False)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['label']),\n",
    "    y=train_df['label']\n",
    ")\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Learning rate scheduler\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(0.01))  \n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=15, \n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# Save the label encoder for future use\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading the test dataset to the model and obtaining predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "test_image_dir = r'D:\\Academics\\IIT\\Tech\\COSMOSOC\\SOI-sds\\test_dataset\\test_dataset'\n",
    "image_size = (224, 224)\n",
    "model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "label_encoder = joblib.load('label_encoder.pkl')\n",
    "\n",
    "test_image_files = [f for f in os.listdir(test_image_dir) if os.path.isfile(os.path.join(test_image_dir, f))]\n",
    "\n",
    "# Preprocessing of the test images\n",
    "test_images = []\n",
    "image_names = []\n",
    "\n",
    "for filename in test_image_files:\n",
    "    image_path = os.path.join(test_image_dir, filename)\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Warning: Could not read image {image_path}. Skipping...\")\n",
    "        continue\n",
    "    image = cv2.resize(image, image_size)\n",
    "    image = image.astype(float) / 255.0\n",
    "    test_images.append(image)\n",
    "    image_names.append(filename)\n",
    "\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "\n",
    "# Predicting labels in batches to avoid memory issues\n",
    "batch_size = 16\n",
    "num_batches = int(np.ceil(len(test_images) / batch_size))\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, len(test_images))\n",
    "    batch_images = test_images[start_idx:end_idx]\n",
    "    batch_predictions = model.predict(batch_images)\n",
    "    all_predictions.append(batch_predictions)\n",
    "\n",
    "\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "\n",
    "predicted_labels = np.argmax(all_predictions, axis=1)\n",
    "\n",
    "# Decoding the numerical labels to categorical class names\n",
    "decoded_labels = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "output_df = pd.DataFrame({'File_Name': image_names, 'Predicted_Label': decoded_labels})\n",
    "output_df.to_csv('test_predictions.csv', index=False)\n",
    "print('Predicted labels stoered in the test_predictions.csv file sucessfuly!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
